{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convert_h5_to_pb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXxS-5VoZ7uu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "# This line must be executed before loading Keras model.\n",
        "K.set_learning_phase(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbrWfWjubAVw",
        "colab_type": "code",
        "outputId": "dd45f81a-328e-40a6-98cd-2bcd6af12100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/Detecting-Pneumonia.h5')\n",
        "print(model.outputs)\n",
        "# [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]\n",
        "print(model.inputs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor 'dense_2/Identity:0' shape=(None, 2) dtype=float32>]\n",
            "[<tf.Tensor 'input_1_2:0' shape=(None, 224, 224, 3) dtype=float32>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyWbNfV-V668",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
        "    \"\"\"\n",
        "    Freezes the state of a session into a pruned computation graph.\n",
        "\n",
        "    Creates a new computation graph where variable nodes are replaced by\n",
        "    constants taking their current value in the session. The new graph will be\n",
        "    pruned so subgraphs that are not necessary to compute the requested\n",
        "    outputs are removed.\n",
        "    @param session The TensorFlow session to be frozen.\n",
        "    @param keep_var_names A list of variable names that should not be frozen,\n",
        "                          or None to freeze all the variables in the graph.\n",
        "    @param output_names Names of the relevant graph outputs.\n",
        "    @param clear_devices Remove the device directives from the graph for better portability.\n",
        "    @return The frozen graph definition.\n",
        "    \"\"\"\n",
        "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
        "    graph = session.graph\n",
        "    with graph.as_default():\n",
        "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
        "        output_names = output_names or []\n",
        "        output_names += [v.op.name for v in tf.global_variables()]\n",
        "        # Graph -> GraphDef ProtoBuf\n",
        "        input_graph_def = graph.as_graph_def()\n",
        "        if clear_devices:\n",
        "            for node in input_graph_def.node:\n",
        "                node.device = \"\"\n",
        "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
        "                                                      output_names, freeze_var_names)\n",
        "        return frozen_graph\n",
        "\n",
        "\n",
        "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Q3s6m8dOs8",
        "colab_type": "code",
        "outputId": "f40383a0-8b19-460f-a286-1b5d4624e405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.write_graph(frozen_graph, \"model\", \"tf_model.pb\", as_text=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model/tf_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwsSVFGudWzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import gfile\n",
        "sess = tf.Session()\n",
        "f = gfile.FastGFile(\"./model/tf_model.pb\", 'rb')\n",
        "graph_def = tf.GraphDef()\n",
        "# Parses a serialized binary message into the current message.\n",
        "graph_def.ParseFromString(f.read())\n",
        "f.close()\n",
        "\n",
        "sess.graph.as_default()\n",
        "# Import a serialized TensorFlow `GraphDef` protocol buffer\n",
        "# and place into the current default `Graph`.\n",
        "tf.import_graph_def(graph_def)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9QvRmOdo02Q",
        "colab_type": "code",
        "outputId": "ecba3153-47ff-4c71-e895-39a635022f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "converter = trt.TrtGraphConverter(\n",
        "\tinput_graph_def=frozen_graph,\n",
        "\tnodes_blacklist=['dense_1/Softmax:0'])\n",
        "frozen_graph = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Linked TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Loaded TensorRT version: (0, 0, 0)\n",
            "INFO:tensorflow:Running against TensorRT version 0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8BUzc8Io5Yx",
        "colab_type": "code",
        "outputId": "587fd07d-efd6-4f95-ca78-3eccaddfe560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.write_graph(frozen_graph,\"inception\",\"tftrt_model.pb\",as_text=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'inception/tftrt_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6keEKtyKImZ",
        "colab_type": "code",
        "outputId": "a41dc158-ba97-4c36-979e-93865c5f265b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "all_nodes=len([1 for n in frozen_graph.node])\n",
        "print(\"no. of all_nodes in frozen graph: \",all_nodes)\n",
        "trt_engine_nodes = len([1 for n in graph_def.node if str(n.op)=='TRTeng'])\n",
        "print(\"no. of trt_engine_nodes in tensorrt graph: \",trt_engine_nodes)\n",
        "all_nodes=len([1 for n in graph_def.node])\n",
        "print(\"no. of all_nodes in tensorrt graph: \",all_nodes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of all_nodes in frozen graph:  517\n",
            "no. of trt_engine_nodes in tensorrt graph:  0\n",
            "no. of all_nodes in tensorrt graph:  1726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gOQpHmck7od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM3oHZGpJTRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "files = os.listdir('chest_xray/test/NORMAL/')\n",
        "img1 = image.load_img(r'chest_xray/test/NORMAL/' + files[0], target_size=(224, 224))\n",
        "img_array1 = image.img_to_array(img1)\n",
        "img_array_expanded_dims1 = np.expand_dims(img_array1, axis=0)\n",
        "input_img = preprocess_input(img_array_expanded_dims1)\n",
        "\n",
        "for i in files[1:64]:\n",
        "    img2 = image.load_img('chest_xray/test/NORMAL/' + i, target_size=(224, 224))\n",
        "    img_array2 = image.img_to_array(img2)\n",
        "    img_array_expanded_dims2 = np.expand_dims(img_array2, axis=0)\n",
        "    img2 = preprocess_input(img_array_expanded_dims2)\n",
        "\n",
        "    input_img = np.concatenate((input_img, img2),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkucSxA8k5GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to read a \".pb\" model \n",
        "# (can be used to read frozen model or TensorRT model)\n",
        "def read_pb_graph(model):\n",
        "  with gfile.FastGFile(model,'rb') as f:\n",
        "    graph_def = tf.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "  return graph_def"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnfU3d0Lnfx",
        "colab_type": "code",
        "outputId": "36915168-b341-4473-a0a1-2ce025b94dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "# variable\n",
        "TENSORRT_MODEL_PATH = '/content/model/tf_model.pb'\n",
        "import time\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.50))) as sess:\n",
        "        # read TensorRT model\n",
        "        trt_graph = read_pb_graph(TENSORRT_MODEL_PATH)\n",
        "\n",
        "        # obtain the corresponding input-output tensor\n",
        "        tf.import_graph_def(trt_graph, name='')\n",
        "        input = sess.graph.get_tensor_by_name('input_1_1:0')\n",
        "        output = sess.graph.get_tensor_by_name('dense_1/Softmax:0')\n",
        "\n",
        "        # in this case, it demonstrates to perform inference for 50 times\n",
        "        total_time = 0; n_time_inference = 50\n",
        "        out_pred = sess.run(output, feed_dict={input:input_img})\n",
        "        for i in range(n_time_inference):\n",
        "            t1 = time.time()\n",
        "            out_pred = sess.run(output, feed_dict={input:input_img})\n",
        "            t2 = time.time()\n",
        "            delta_time = t2 - t1\n",
        "            total_time += delta_time\n",
        "            print(\"needed time in inference-\" + str(i) + \": \", delta_time)\n",
        "        avg_time_tensorRT = total_time / n_time_inference\n",
        "        print(\"average inference time: \", avg_time_tensorRT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "needed time in inference-0:  0.15371346473693848\n",
            "needed time in inference-1:  0.1522524356842041\n",
            "needed time in inference-2:  0.15550637245178223\n",
            "needed time in inference-3:  0.15519475936889648\n",
            "needed time in inference-4:  0.154343843460083\n",
            "needed time in inference-5:  0.15735530853271484\n",
            "needed time in inference-6:  0.15699219703674316\n",
            "needed time in inference-7:  0.15268540382385254\n",
            "needed time in inference-8:  0.15582847595214844\n",
            "needed time in inference-9:  0.1558988094329834\n",
            "needed time in inference-10:  0.15273475646972656\n",
            "needed time in inference-11:  0.15705513954162598\n",
            "needed time in inference-12:  0.15645337104797363\n",
            "needed time in inference-13:  0.15340852737426758\n",
            "needed time in inference-14:  0.15452790260314941\n",
            "needed time in inference-15:  0.15575480461120605\n",
            "needed time in inference-16:  0.15274858474731445\n",
            "needed time in inference-17:  0.15604901313781738\n",
            "needed time in inference-18:  0.15709829330444336\n",
            "needed time in inference-19:  0.15480709075927734\n",
            "needed time in inference-20:  0.15466976165771484\n",
            "needed time in inference-21:  0.15671944618225098\n",
            "needed time in inference-22:  0.1533823013305664\n",
            "needed time in inference-23:  0.1548175811767578\n",
            "needed time in inference-24:  0.15571069717407227\n",
            "needed time in inference-25:  0.15404009819030762\n",
            "needed time in inference-26:  0.15453672409057617\n",
            "needed time in inference-27:  0.1582949161529541\n",
            "needed time in inference-28:  0.15607190132141113\n",
            "needed time in inference-29:  0.1547400951385498\n",
            "needed time in inference-30:  0.1585986614227295\n",
            "needed time in inference-31:  0.15477728843688965\n",
            "needed time in inference-32:  0.1557784080505371\n",
            "needed time in inference-33:  0.1544342041015625\n",
            "needed time in inference-34:  0.15844011306762695\n",
            "needed time in inference-35:  0.1569221019744873\n",
            "needed time in inference-36:  0.15573954582214355\n",
            "needed time in inference-37:  0.15488553047180176\n",
            "needed time in inference-38:  0.15578222274780273\n",
            "needed time in inference-39:  0.15580368041992188\n",
            "needed time in inference-40:  0.15564966201782227\n",
            "needed time in inference-41:  0.15836691856384277\n",
            "needed time in inference-42:  0.15621423721313477\n",
            "needed time in inference-43:  0.15529441833496094\n",
            "needed time in inference-44:  0.15445804595947266\n",
            "needed time in inference-45:  0.15788745880126953\n",
            "needed time in inference-46:  0.15688037872314453\n",
            "needed time in inference-47:  0.1557633876800537\n",
            "needed time in inference-48:  0.15666508674621582\n",
            "needed time in inference-49:  0.15917181968688965\n",
            "average inference time:  0.1556181049346924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWwnR4TppldO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eylNCz7ysSvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}